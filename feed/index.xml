<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://y.tsutsumi.io/feed/index.xml" rel="self" type="application/atom+xml" /><link href="https://y.tsutsumi.io/" rel="alternate" type="text/html" /><updated>2026-01-17T00:05:15+00:00</updated><id>https://y.tsutsumi.io/feed/index.xml</id><title type="html">Yusuke Tsutsumi</title><subtitle>My blog on software, productivity, and obsessively optimizing. I work at Google, ex-Zillow. Thoughts my own.</subtitle><entry><title type="html">Review on the 2025 Framework 13</title><link href="https://y.tsutsumi.io/framework-13-2025" rel="alternate" type="text/html" title="Review on the 2025 Framework 13" /><published>2026-01-10T07:00:00+00:00</published><updated>2026-01-10T07:00:00+00:00</updated><id>https://y.tsutsumi.io/framework-laptop</id><content type="html" xml:base="https://y.tsutsumi.io/framework-13-2025"><![CDATA[<p><img src="./2026-01-10-framework-laptop.png" alt="The framework laptop 13" /></p>

<p>I finally got a <a href="https://frame.work/">Framework</a> 13 laptop! This is something I’ve wanted for several years, ever since they started shipping their laptops in 2021.</p>

<h1 id="a-quick-intro-to-framework">A quick intro to framework</h1>

<p>Framework focuses on repairable, but also modular, devices. They have produced laptops in the 12,13, and 16 inch form factor, as well as a desktop. Some of the features that are specific to Framework devices include:</p>

<p>“swappable I/O”: effectively there are slots of USB-C ports that have a square notch in them, paired with adapters to various common ports like USB-A, DisplayPort, or HDMI. The result is the user-facing ports are effectively changeable, to some extent on the fly: it’s hard to change the port in, say, a second, but if you had 20-30 seconds I think you could change them out.</p>

<p>Repairable / interchangeable parts: there is a whole Framework store with replacement parts for everything from batteries to webcams to keyboards. You can change things like the bezel color around your monitor, or for the desktop they have square chicklets that you can replace with your own flair.</p>

<h1 id="what-motivated-me">What motivated me</h1>

<p>So why the wait? Well, at the time I had a working laptop from 2018 that was working perfectly okay. It still even works okay (battery has only degraded to maybe 80% or so at most).</p>

<p>So in 2025, there’s a couple things that have changed for me:</p>

<ul>
  <li>The availablility of RISC-V and ARM boards especially.</li>
  <li>The AI HX 370 processor, released this year.</li>
</ul>

<h1 id="impressions-and-thoughts">Impressions and thoughts</h1>

<p>Overall, I’m very impressed and happy with my 13. I bought the “DIY” version, which require some self-installation of the RAM, hard drive, but I preferred that as it gave me time to quickly get a more visceral sense of the device.</p>

<p>It’s also my first laptop with the 3:2 display, which runs taller than the ratios typically seen in laptops these days. I do feel like the extra vertical real estate is noticeable, and helps a bit with coding or text-based tasks.</p>

<p>I’m also happy with the performance: the device feels extremely snappy with Ubuntu 25.10 and the standard Gnome display manager. Videos and tabs load quickly. Aside from games there is no percievable latency as far as processing is concerned.</p>

<p>Of course, my frames of reference are an M2 MacBook Pro from 2022, and an HP Spectre X360 from 2018, but compared to both of these my laptop feels much smoother.</p>

<p>Battery life also seems to be holding quite well - I charge pretty aggresively regardless, but even for heavy workloads like gaming with 100% GPU utilization I’m looking at 1-2 hours. If it’s more lightweight work than that I have not yet thought about battery at all, but I’ll probably do some more emperical benchmarks soon. <code class="language-plaintext highlighter-rouge">powertop</code> reports a draw of 7 watts or so at this time, so I’m guessing for light work I can get about 7 or 8 hours.</p>

<p>The build of the laptop is also very nice - The aluminum feels very similar to a Macbook, but the device feels much lighter than the M2 PRO. This feels like a “premium” laptop to me.</p>

<h2 id="availability-for-risc-v-and-arm-boards">Availability for RISC-V and ARM boards</h2>

<p>The amazing thing about Framework devices are not just repairability - it’s the modularity as well.</p>

<p>The 13 form factor in particular has been so successful that other vendors have started producing compatible boards:</p>

<ul>
  <li><a href="https://frame.work/blog/risc-v-mainboard-for-framework-laptop-13-is-now-available">DeepComputing’s RISC-V board</a></li>
  <li><a href="https://metacomputing.io/products/metacomputing-aipc">MetaComputing’s ARM-based board</a>.</li>
</ul>

<p>Improving support of alternative processors seems like an amazing hobby project. Especially ARM with the recent announcement of it’s integration of <a href="https://fex-emu.com/">FEX</a>-based emulation of X86 to play steam games on ARM devices.</p>

<h2 id="the-ai-9-hx-370">The AI 9 HX 370</h2>

<p>I’ve been angling for the Framework 16 in particular because of the higher TDP, ensuring that I can use the maximum potential of any hardware I have for the device. This was also to ensure I could upgrade the GPU as more resource-intensive games are released.</p>

<p>However, the AI HX 390 seems to have a pretty amazing iGPU, with the Radeon 890m. In my benchmarks I’m getting good <em>enough</em> framerates to accept it on the off change I’m gaming on the go. Note that these are using single-channel memory so the GPU can likely perform much, much better:</p>

<ul>
  <li>30-40 FPS with Cyberpunk 2077.</li>
  <li>30 FPS with Balder’s Gate 3.</li>
  <li>30 FPS with Expedition 33 (with some really low resolution settings).</li>
</ul>

<p>Some more lightweight games like Dispatch run at 60fps, just as good as my desktop.</p>

<h3 id="for-ai">For AI</h3>

<p>I’ve recently started dabling with ML model architecture and training. My <a href="https://github.com/toumorokoshi/yft-ml-sandbox/tree/main/alexnet">reproduction of the AlexNet paper</a> on the Imagenette dataset trains 60 epochs in about 3-4 hours. This isn’t nearly as fast as the 30 minutes on the <a href="https://docs.cloud.google.com/compute/docs/gpus?hl=ja#l4-gpus">L4 GPUs available through Google Cloud</a> that I normally test against, but it’s still a decent showing, especially limited at the 30w TDP.</p>

<h2 id="mistake-buying-single-channel-memory">Mistake: buying single-channel memory</h2>

<p>Honestly I wasn’t thinking straight on this one. I bought 1 32GB sodimm of RAM, thinking that I’ll buy a second one when the RAM prices go down (the current AI trend has caused RAM prices to more than triple). But the single sodimm really hurts any memorybound operations. In particular AI training/inference and gaming: two things I’m doing a fair bit right now. For gaming I’ve read benchmarks that dual dimms can up the framerate of some games by about 10-20%.</p>

<p>Even if I buy a second one, I believe I will end up with a higher amount of ram anyway (probably maxing out at 128GB if possible, past the theoretically supported 96GB for the AI 9 HX 370).</p>

<p>I’m considering buying a second smaller DIMM the first 16GB could be dual-channel, but then I would rely on how functional the processor is load-balancing across uneven sizes of RAM.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Reflecting on 2025</title><link href="https://y.tsutsumi.io/my-year/2025" rel="alternate" type="text/html" title="Reflecting on 2025" /><published>2026-01-05T07:00:00+00:00</published><updated>2026-01-05T07:00:00+00:00</updated><id>https://y.tsutsumi.io/my-year/reflecting-on-2025</id><content type="html" xml:base="https://y.tsutsumi.io/my-year/2025"><![CDATA[<p>With the end of the year, it’s time for another reflection.</p>

<h2 id="staying-focused">Staying focused</h2>

<p>Overall, I’ve been much better about staying focused on various projects this year. One major contributor is not splitting my focus, and accepting that things take a lot longer than I expect. Whether it’s personal projects like thre AEPs, a new task at work, or a video game I want to play, trying to rush through leads to less satisfaction personally, and my accomplishments not being as deep on things as I’d like.</p>

<h2 id="farsi">Farsi</h2>

<p>If I had to be honest, my Farsi may have actually gotten worse this year. I’m still trying to learn the language, but I’m not doing classes weekly as much anymore, and I dropped Children of Morta due to a bunch of work projects that made me work a few late nights.</p>

<p>I also haven’t really been watching Persian TV, which may be another contributing factor.</p>

<h2 id="oss-aepdev">OSS: aep.dev</h2>

<p>aep.dev this year has been great! We <a href="https://aep.dev/blog/aep-2026-release/">finally published our 2026 specification</a>, a culmination of several years of blood, sweat, and tears discussing API design.</p>

<p>This was joint effort from multiple people who were involved in the ecosystem, including Alex Stephen, Marsh Gardiner, Mike Kistler, David Gagne, and more. It’s a privilege to be a part of that ecosystem.</p>

<p>With the specification underway, I’m excited to move more into actionable territory, taking the spec to organizations to hear what they think.</p>

<h2 id="fitness">Fitness</h2>

<p>Between 2024 and 2025, I’ve lost a couple pounds of fat, arriving at roughly 12% body fat (via dexa), which I’ve largely tracked for most of the year.</p>

<p>Keeping that percentage, I was able to even gain a couple of pounds from where I was at in the beginning of 2025.</p>

<p>At roughly 170lbs (via morning weigh-in), or 175lbs according to Dexa, I think I’m at the point where any more weight loss may result in loss of muscle that is undesirable.</p>

<p>Theoretically, advanced lifters can gain 3-4 pounds of mass a year, maximum. If I am able to gain even half of that while maintaining the current weight, I would be at 10.8% body fat, and would have achieved my goal.</p>

<p>So, my focus this year is to maintain, but try to gain as much muscle as I can.</p>

<p>I upped my protein intake temporarily to 1.8g / kg, but after I saw my Kidney’s health marker (EGFR) lower, I decided to lower intake back down to 1.8, and also go exclusively plant based (which research has shown has resulted in better kidney health overall).</p>

<p>The EGFR marker with creatinine is known to not be the most reliable alone as it generally increases linearly with protein intake, but nonetheless I wanted to be careful about overall health and not increase my protein intake unless it seems like my creatinine numbers can be maintained.</p>

<p>My bone density also significantly went down this year - I think it’s because I removed olympic lifts and heavy compounds such as deadlifts and squats. I plan to re-add these and check bone density afterward.</p>

<h2 id="piano">Piano</h2>

<p>A surprising win this year was Piano. Near the end of the year, I started practicing some holiday pieces after that Thanksgiving holiday. I’ve found that I can read pieces okay now, and generally play through a piece without much trouble, even if that is nowhere close to real speed.</p>

<p>I think It’s a matter of impetus at this point: just keep practing, and I should be a pretty good player and can sight read without issue by the end of the year.</p>

<h2 id="goals-for-2026">Goals for 2026</h2>

<ul>
  <li>Do 5 talks to evagnelize aep.dev (that would be a lot!)</li>
  <li>Continue to play piano 10 minutes a day. I think I’m getting pretty good, so I’d like to keep that up.</li>
  <li>Achieve 11% body fat (same as last year)</li>
  <li>Gain a working knowledge of machine learning, and figure out how to meaningfully contribute to the ecosystem.</li>
  <li>Become actually conversationally fluent at Farsi
    <ul>
      <li>Finish children of Morta.</li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="reflection" /><summary type="html"><![CDATA[With the end of the year, it’s time for another reflection.]]></summary></entry><entry><title type="html">Rebuilding AlexNet</title><link href="https://y.tsutsumi.io/alexnet" rel="alternate" type="text/html" title="Rebuilding AlexNet" /><published>2025-12-26T07:00:00+00:00</published><updated>2025-12-26T07:00:00+00:00</updated><id>https://y.tsutsumi.io/rebuilding-alexnet</id><content type="html" xml:base="https://y.tsutsumi.io/alexnet"><![CDATA[<h1 id="rebuilding-alexnet">Rebuilding AlexNet</h1>

<p>My role at GM (aka ex-Cruise) has started pulling me and more and more into the machine learning space, in particular models that are critical to Autonomous vehicles such as perception, planning, world models, etc. For context, I have traditionally worked more on the system performance / vehicle software side, not the models directly.</p>

<p>As such, I’ve started to try to get a better grasp of ML fundamentals. One of my first forays into getting hand-on experience is rebuilding <a href="https://en.wikipedia.org/wiki/AlexNet">AlexNet</a>, one of the most influential models to modern machine learning. It was the first model to get a top-5 error rate of 15.3% in the ImageNet Large Scale Recognition Challenge and using convolutional neural networks in the process.</p>

<p>This post serves to document some of my learnings as I attempted to recreate it.</p>

<h1 id="my-general-approach">My General Approach</h1>

<p>I didn’t start by recreating AlexNet immediately: I instead started with
a simple neuralnet + RELU based model, to try to understand the individual impact of various model architectures.</p>

<p>I also tried to use vanilla PyTorch, although I did build a <a href="https://github.com/toumorokoshi/yft-ml-sandbox/tree/main/alexnet">bazel-based build workflow</a> since I prefer the hermicity and relatively simple dependency management of multiple languages.</p>

<p>To keep the training fast (since I was focused more experimentation rather than production use cases), I used the much smaller <a href="https://github.com/fastai/imagenette">imagenette</a> dataset rather than ImageNet which AlexNet was trained and tested against. This has only 10 classifications, as well as less data.</p>

<h1 id="the-learnings">The Learnings</h1>

<h2 id="terminology">Terminology</h2>

<p>If I had one big takeaway, it was understanding the terminology a lot more deeply! Knowing the terms logit, epoch, stochastic gradient descent, maxpool, generalization, and overfitting in a more intuitive and visceral helps me understand ideas discussed within our AV organization much more quickly.</p>

<h2 id="always-randomize-datasets">Always randomize datasets</h2>

<p>result: 9.99% -&gt; 40%</p>

<p>My initial iteration had a 9.99% pass rate, after adding the debugger and looking at the predicted output of the model by printing the final logit layer, I noticed that the output was always a specific classification, no matter what the input was.</p>

<p>This was caused by me not randomizing the dataset - effectively every epoch, the model would get exclusively trained against one classification for the last 10% of the time, biasing it toward that one classification.</p>

<p>After I fixed that, my simple 2-layer neural network got up to 40% accuracy!</p>

<h2 id="misc-things-that-helped-the-neural-net">Misc things that helped the neural net</h2>

<p>The highest I could get the neural network was 44%. Here’s a table of things I tried:</p>

<table>
  <thead>
    <tr>
      <th>Accuracy</th>
      <th>Epochs</th>
      <th>Batch Size</th>
      <th>Learning Rate</th>
      <th>Shuffled</th>
      <th>Momentum</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>44.6%</td>
      <td>10</td>
      <td>32</td>
      <td>0.01</td>
      <td>yes</td>
      <td>0</td>
    </tr>
    <tr>
      <td>42.9%</td>
      <td>20</td>
      <td>16</td>
      <td>0.01</td>
      <td>yes</td>
      <td>0</td>
    </tr>
    <tr>
      <td>41.9%</td>
      <td>10</td>
      <td>16</td>
      <td>0.001</td>
      <td>yes</td>
      <td>0</td>
    </tr>
    <tr>
      <td>40.9%</td>
      <td>10</td>
      <td>32</td>
      <td>0.001</td>
      <td>yes</td>
      <td>0</td>
    </tr>
    <tr>
      <td>38.0%</td>
      <td>5</td>
      <td>16</td>
      <td>0.001</td>
      <td>yes</td>
      <td>0</td>
    </tr>
    <tr>
      <td>11%</td>
      <td>10</td>
      <td>16</td>
      <td>0.01</td>
      <td>no</td>
      <td>0.9</td>
    </tr>
    <tr>
      <td>9.9%</td>
      <td>10</td>
      <td>16</td>
      <td>0.001</td>
      <td>no</td>
      <td>0</td>
    </tr>
  </tbody>
</table>

<p>I could tweak batch size and learning rate to help it converge more quickly.</p>

<h2 id="single-layer-convolution">Single-layer convolution</h2>

<p>Switching to even a single-layer of convolution immediately caused the model to jump to 55%: as explained in the AlexNet paper, I think this can primarily be attributed to better generalization of the learnings in one particular segment of the image: without convolution and some form of feature consolidation, it’s really impossible to divine general relationships that shift across the matrix like the position of a dog in the image.</p>

<h2 id="60-epochs-is-the-good-range">60+ epochs is the good range.</h2>

<p>For most of the convolution models, many had some continued gain up to about 45 to 60 epochs. I believe this will be highly dependent on how much training data is available, but it’s interesting to thing that running the model against the same data 40+ times is required to hit some sort of ceiling.</p>

<h2 id="dropout-added-10">dropout added 10%</h2>

<p>The original AlexNet paper suggests that introducing dropout in the final connected layer (2 layers of linear + ReLU combinations) results in a significant raining improvement. They theorize this is due to the individual “neurons” in the matrices start to embed more generalization into themselves, not requiring the constant presence of the upstream neurons.</p>

<p>In my experience, this increased the accuracy by roughly 10% from the non-dropout version. So clearly, the neurons embedding more information is a critical part to better models.</p>

<h2 id="flipping-image-data-added-10-accuracy">flipping image data added 10% accuracy</h2>

<p>There isn’t much more I could do with model architecture to increase the accuracy, but I got some pointers to go look at data.</p>

<p>The biggest contributor was just flipping the image data half the time: I think this effectively doubles the dataset to some extent. Which implies that training data might be a bottleneck now rather than just learning capacity.</p>

<p>Some other things I tried, which has less of an effect:</p>

<ul>
  <li>Scaling the data up and cropping didn’t seem to improve much. Maybe 1-2% at most.</li>
  <li>Changing exposure, brightness also had maybe a 1-2% difference, although training still ended up in the same range.</li>
</ul>

<h2 id="my-current-score-78">my current score: 78%</h2>

<p>For the imagenette, I was able to get a 78% score after 60 epochs. This is pretty good, as the original AlexNet paper, with a much larger dataset and longer training time, reached 85%.</p>

<p>I did not see any overfitting directly (which I’d expect to see via loss curve that has an upward trend after multiple epochs). But I did see the loss plateau after about 45 epochs or so.</p>

<p>Some quick notes:</p>

<ul>
  <li>Adding RMSNorm in the first 2 layers added roughly 1-2%.</li>
  <li>Originally I had only convolutional layers with stride=2 rather than maxpool. With the maxpool layers instead I had a 4-5% bump.</li>
</ul>]]></content><author><name></name></author><category term="coding" /><summary type="html"><![CDATA[Rebuilding AlexNet]]></summary></entry><entry><title type="html">How I code with AI</title><link href="https://y.tsutsumi.io/coding-with-ai" rel="alternate" type="text/html" title="How I code with AI" /><published>2025-11-08T07:00:00+00:00</published><updated>2025-11-08T07:00:00+00:00</updated><id>https://y.tsutsumi.io/my-ai-best-practices</id><content type="html" xml:base="https://y.tsutsumi.io/coding-with-ai"><![CDATA[<h1 id="how-i-code-with-ai">How I code with AI</h1>

<p>Recently, like many developers, I’ve been experimenting a lot with coding with AI.</p>

<p>Overall I’ve found that I am more productive with AI: some PRs are vastly easier, although I do write a decent bit myself (especially with https://aep.dev/, which is basically an english specification).</p>

<p>I’ll update this page over time as I learn some new tricks as well.</p>

<h2 id="adding-a-personal-coding-styleguide">Adding a personal coding styleguide</h2>

<p>Most editors provide a way to add a custom styleguide. vscode calls these “instructions”.</p>

<p>My personal instructions are here:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
applyTo: '**'
---
# yft's personal coding guidelines
- prefer fewer comments. Only add them when the code is not self-explanatory.
- prefer functional programming.
- IO with network and filesystem should be in wrapper functions that call inner functions that work on data structures.
- unit tests should be written to run on the data structures directly rather than through IO, with only a single test on IO to serve as an integration test.
- prefer composition over inheritance.
- prefer to use constants, especially for hard-coded directories or values.
- helper functions should have minimal logic.
</code></pre></div></div>

<h2 id="one-off-experiences">One-off experiences</h2>

<h3 id="2025-11-10-using-jules-and-github-copilot">2025-11-10: using Jules and GitHub copilot</h3>

<p>I had trying Jules (https://jules.google/docs/) this weekend on a personal project. It’s kind of similar to this so thought it was worth bringing up.</p>

<p>Basically I had it write a PR to create a new proto schema for me: https://github.com/aep-dev/aep-components/pull/41. The workflow is you add a label “jules” to a GitHub issue, and it goes and generates a PR.</p>

<p>The first pass (like many AI tools) was ok, but had a lot of changes that needed. Many were me not being clear about the initial expectations. But one major hangup was that jules didn’t understand that there was a github CI workflow that it should have run to find more errors. I tried to explain it here, but gave up because it was still failing and I was moving toward comments that just hand-fixed it: https://github.com/aep-dev/aep-components/pull/41#issuecomment-3506923393</p>

<p>I actually tried the exact same change with the built-in copilot generator, which was much closer: https://github.com/aep-dev/aep-components/pull/44. It seemed to better understand the linting and ci workflows, and didn’t have the same breakages with buf.</p>

<p>My workflow was still leaving a bunch of comments, many of which are direct code fixes. But I was able to massage and merge it in.</p>

<p>Ultimately I think this would have been faster for me to have just done locally with cursor / copilot, then hand-fix and submit the PR myself, then have a PR generator do it. But it felt like needing a fundamental understanding of the tooling in the repo was critical.</p>]]></content><author><name></name></author><category term="programming" /><summary type="html"><![CDATA[How I code with AI]]></summary></entry><entry><title type="html">The Anxious Generation by Jonathan Haidt</title><link href="https://y.tsutsumi.io/books/anxious-generation" rel="alternate" type="text/html" title="The Anxious Generation by Jonathan Haidt" /><published>2025-11-02T07:00:00+00:00</published><updated>2025-11-02T07:00:00+00:00</updated><id>https://y.tsutsumi.io/books/the-anxious-generation</id><content type="html" xml:base="https://y.tsutsumi.io/books/anxious-generation"><![CDATA[<h1 id="the-anxious-generation">The Anxious Generation</h1>

<p>I really liked reading the book “The Anxious generation” by Jonathan Haidt.</p>

<p>The book argues for less screen time for children, especially social media at
younger ages, which can be harmful.</p>

<p>The jist of the book is:</p>

<p>1: The time your child spends outside or interacting with kids is more valuable that almost all
forms of screen time. I like this argument because it’s a relative comparison, rather than saying
that screen time is inherently bad (which, as someone who spend his whole childhood in front a
screen and turned out… not terrible, tend to disagree).</p>

<p>2: Social media for young children can be a terrible infleunce. There’s cyberbullying, which can be
much worse than regular bullying as people in general are willing to be a lot more horrible online.
In addition, there’s the implicit pressure to live and look as glamorous as these famous influencers
online, resulting in body dysmoprhia.</p>

<p>Although the book references some studies, I agree on an anecdotal level that social media is not a
necessary part of life for adolescents, and that screentime is something with, depending on the
activity, rapidly diminishing returns, and a balance with other activities in life is critical.</p>]]></content><author><name></name></author><category term="book-reports" /><summary type="html"><![CDATA[The Anxious Generation]]></summary></entry><entry><title type="html">Setting up a streaming gaming Ubuntu PC for sunshine/moonlight</title><link href="https://y.tsutsumi.io/ubuntu-sunshine/" rel="alternate" type="text/html" title="Setting up a streaming gaming Ubuntu PC for sunshine/moonlight" /><published>2025-07-18T07:00:00+00:00</published><updated>2025-07-18T07:00:00+00:00</updated><id>https://y.tsutsumi.io/setting-up-moonlight-ubuntu</id><content type="html" xml:base="https://y.tsutsumi.io/ubuntu-sunshine/"><![CDATA[<h1 id="setting-up-a-streaming-gaming-ubuntu-pc-for-moonlight">Setting up a streaming gaming Ubuntu PC for moonlight</h1>

<p>I’ve been working on building a new streaming gaming PC (that ordeal I will save
for a future blog post), but in the process I had some difficulties with getting
moonlight set up just the way I like it. So I figured these might be helpful to
others.</p>

<p>My goal is to have a PC that I use exclusively for streaming games via
<a href="https://app.lizardbyte.dev/Sunshine/?lng=en-US">sunshine</a>. Sunshine is the
replacement for Nvidia game streaming that was deprecated, and work as a
complement to the streaming client <a href="https://moonlight-stream.org/">moonlight</a>.
It allows for the streaming of video and audio to a client, which in turn
streams controls (keyboard and / or a controller) back to the server.</p>

<p>When all is said and done, I want what is effectively a box in my closet with
only two things attached: a power cable, and an ethernet cable (for a
low-latency wired connection to my network). I do not want dangling mice,
keyboards or monitors. Just a box that connects to the internet, and is
available for me to stream from.</p>

<p>To accomplish this, I needed a few things:</p>

<ol>
  <li>A way to have a virtual display even if a physical monitor isn’t plugged in.</li>
  <li>A way to login as a user right away (since sunshine needs a valid user).</li>
  <li>A way to have sunshine available when the machine boots.</li>
  <li>A way to wake the machine after it suspends</li>
</ol>

<h2 id="attaching-a-virtual-display">Attaching a virtual display</h2>

<p>This was relatively easy!</p>

<p>You can buy a <a href="https://www.google.com/search?q=dummy+plug+hdmi">dummy plug</a> that
you just plug into the GPU. When you’ve got your PC set up, just replace your
actual monitor with that.</p>

<h2 id="logging-in-right-away">Logging in right away</h2>

<p>This one just requires careful setup:</p>

<ol>
  <li>setup autologin in your operating system (varies by distro).</li>
  <li><em>do not</em> use disk encryption.</li>
</ol>

<p>(2) is the major gotcha: if you add disk encryption it’ll ask for your password
every time your computer starts up, which means plugging in a keyboard
every time. you could just leave a keyboard by your computer for this purpose,
but I’ve found I do have to restart my computer from time to time, and making
bringing it back up as easy as pressing a button is a nice usability improvement.</p>

<h2 id="start-sunshine-when-the-machine-boots">Start sunshine when the machine boots</h2>

<p>This was pretty hard for me. Normally, this is a matter of installing the
sunshine .deb from the <a href="https://github.com/LizardByte/Sunshine/releases">github
releases</a>, but in my case I had
to use Ubuntu 25.05 for a newer kernel version that supports my AMD GPU. Only
LTS release are supported for sunshine.</p>

<p>For a non-lts release, I found that using the <code class="language-plaintext highlighter-rouge">.appimage</code> worked the best. This
unfortunately didn’t work for me without root, so I had to run sunshine as sudo:</p>

<p><code class="language-plaintext highlighter-rouge">sudo -i PULSE_SERVER=unix:/run/user/$(id -u $whoami)/pulse/native /usr/bin/sunshine</code></p>

<p>(I moved the appimage to /usr/bin/sunshine).</p>

<p>I put this in a script, which I then added as a <a href="https://help.ubuntu.com/stable/ubuntu-help/startup-applications.html.en">startup application</a>:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/usr/bin/env bash</span>
<span class="c"># I had to add a sleep to let the graphical interface completely come up.</span>
<span class="c"># otherwise I would see a black screen.</span>
<span class="nb">sleep </span>10
<span class="nb">exec sudo</span> <span class="nt">-i</span> <span class="nv">PULSE_SERVER</span><span class="o">=</span>unix:/run/user/<span class="si">$(</span><span class="nb">id</span> <span class="nt">-u</span> <span class="nv">$whoami</span><span class="si">)</span>/pulse/native /usr/bin/sunshine
</code></pre></div></div>

<p>This works, but still has some wonkiness:</p>

<ul>
  <li>steam can’t be launched via moonlight because it tries to start it as root.</li>
</ul>

<h2 id="a-way-to-wake-the-machine-after-it-suspends">A way to wake the machine, after it suspends</h2>

<p>Now you probably don’t want your machine on all the time - instead, you’d want
it to suspend after a reasonable idle time frame, and have the ability to turn
it one when you need it.</p>

<p>I tried a few approach, none of which worked or were reliable (I think something
else was resetting my network configuration):</p>

<ul>
  <li><a href="https://wiki.archlinux.org/title/Wake-on-LAN">adding a rule to /systemd/network</a></li>
  <li>Adding a vanilla systemd unit to run ethtool.</li>
</ul>

<p>Ultimately I landed on the following:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># /lib/systemd/system/wol@.service
[Unit]
Description=Wake-on-lan for %i
# This means run after the network is online, as well
# as after suspension.
After=network-online.target suspend.target

[Service]
Type=oneshot
# something else in my OP keeps setting the WOL settings on the device.
# adding this sleep gave whatever time to configure it's settings, so I could override them.
ExecStartPre=/usr/bin/sleep 10
ExecStart=/sbin/ethtool -s %i wol g

[Install]
# if we run network-online and suspend.target, run this too.
WantedBy=network-online.target suspend.target
</code></pre></div></div>

<p>Installed by:</p>

<p><code class="language-plaintext highlighter-rouge">systemctl enable wol@${ethid}</code></p>

<p>Where <code class="language-plaintext highlighter-rouge">${ethid}</code> is the interface id you can get from <code class="language-plaintext highlighter-rouge">nmcli</code>.</p>

<h2 id="conclusion">Conclusion</h2>

<p>And that’s it! Most of it was troubleshooting systemd units. My takeaways are:</p>

<ul>
  <li>try to use systemd units as much as possible. It’s an extremely powerful
system and enables you to start your process with the right sequencing and situations.</li>
  <li>try to use LTS releases of Ubuntu if at all possible. Moonlight installation
has been tricky, and I’m sure other packages only support LTS. I’m looking
forward to upgrading to 2026.04 precisely for this benefit.</li>
</ul>

<h3 id="why-linux-why-ubuntu">Why linux? Why ubuntu?</h3>

<p>Although not for everyone, I like to use Linux for a few reasons:</p>

<ol>
  <li>Linux distributions don’t to forced updates, thereby allowing gaming
uninterrupted by some malware update.</li>
  <li>Windows has a ton of bloatware and ads.</li>
  <li>Linux is “free” - it doesn’t cost a cent to run it, and distros often work
for years.</li>
  <li>With the work done on <a href="https://www.winehq.org/">wine</a> and
<a href="https://github.com/ValveSoftware/Proton">proton</a>, Windows games run
extremely smooth on Linux now.</li>
</ol>

<p>For Ubuntu, I would go with <a href="https://bazzite.gg/">bazzite</a> for a pure gaming PC,
but as this computer will be the most powerful one I have, I wanted it to have
the option to be more general purpose in case I get the itch to do some real
software development on it (maybe ML or some CPU emulation project).</p>]]></content><author><name></name></author><category term="computers" /><summary type="html"><![CDATA[Setting up a streaming gaming Ubuntu PC for moonlight]]></summary></entry><entry><title type="html">OKRs and goal setting</title><link href="https://y.tsutsumi.io/okrs/" rel="alternate" type="text/html" title="OKRs and goal setting" /><published>2025-07-11T07:00:00+00:00</published><updated>2025-07-11T07:00:00+00:00</updated><id>https://y.tsutsumi.io/okrs-and-goal-setting</id><content type="html" xml:base="https://y.tsutsumi.io/okrs/"><![CDATA[<h1 id="okrs-and-goal-setting">OKRs and goal setting</h1>

<p>I’ve had a lot of conversations around OKRs, so I wanted to write down my
thoughts around them.</p>

<h2 id="what-are-okrs-and-how-are-they-defined">What are OKRs, and how are they defined</h2>

<p>OKRs stand for “objectives” and “key results”.</p>

<p>OKRs were developed by Andy Grove at Intel in the 1970s and later popularized by
John Doerr, who learned about them while working at Intel, and then introduced
them to Google in 1999. The framework has since been adopted by many technology
companies and organizations worldwide.</p>

<p><strong>Objectives</strong> are qualitative, inspirational goals that answer the question “What do we want to achieve?” They should be:</p>
<ul>
  <li>Clear and specific</li>
  <li>Time-bound (typically quarterly)</li>
  <li>Ambitious but achievable</li>
  <li>
    <h2 id="aligned-with-company-strategy">Aligned with company strategy</h2>
    <p>Objectives create alignment across teams and drive focus on the most important
priorities for the organization.</p>
  </li>
</ul>

<p><strong>Key Results</strong> are quantitative, measurable outcomes that answer the question “How will we know if we achieved our objective?” They should be:</p>
<ul>
  <li>Specific and measurable</li>
  <li>Time-bound</li>
  <li>Aggressive but realistic</li>
  <li>Leading indicators of success</li>
</ul>

<p>Key results provide a quantitative measure of an objective: if all of the KRs
are reached, the objective is considered accomplished.</p>

<h2 id="hierarchy-of-okrs">Hierarchy of OKRs</h2>

<pre><code class="language-mermaid">graph TD
    O[Objective: Improve Customer Satisfaction]
    KR[Key Result: Achieve 90% Customer Satisfaction Score]
    P[Project: Implement Customer Feedback System]

    P --&gt; KR --&gt; O
</code></pre>

<p>This hierarchy shows how:</p>
<ul>
  <li><strong>Objectives</strong> (top level) define what you want to achieve</li>
  <li><strong>Key Results</strong> (middle level) measure progress toward objectives</li>
  <li><strong>Projects</strong> (bottom level) are the actual work initiatives that drive key results</li>
</ul>

<h2 id="addressing-okr-concerns">Addressing OKR Concerns</h2>

<p>I’ve often heard from many ICs and technical managers that OKRs feel like a
pedantic exercise that only serves as busywork, and doesn’t provide real value
to the organization, or might constrain the scope of the work unnecessarily. I
hope to argue that, with the right process, OKRs can be neither.</p>

<h3 id="okrs-create-a-common-language-of-communication">OKRs create a common language of communication</h3>

<p>In my experience working with senior leadership teams, the biggest challenge in
communication comes from <em>not articulating engineering projects in terms of a
measure that leadership can truly appreciate</em>. This requires a mindset shift,
especially for engineers who are used to dealing with problems that require deep
context to solve and fully appreciate.</p>

<p>Senior leaders often have dozens, if not hundreds of members in their
organization. They will likely not have the bandwidth to intimately understand
the work you’re doing, although they probably want to. This puts an individual
engineer or a team at a risk: if there is a disconnect in context and a common
language across the two levels, the engineer may be working on amazing things
that go unrecognized, and the leader may miss the opportunity to give critical
feedback on the alignment of goals. OKRs serve as that missing common language,
helping frame the technical work being done in terms of the impact to the
organization and a quantitative way to measure it.</p>

<h3 id="concern-okrs-are-too-heavyweight">Concern: OKRs are too heavyweight</h3>

<p>When there is feedback that the OKR process is busywork, I think a lot of that
comes from a heavyweight OKRs process. Including:</p>

<ul>
  <li>A process that forces very stringent requirements and arguing pedantries, like
requiring specific phrasing or formatting.</li>
  <li>A process that stretches weeks at a time, where the next OKR planning begins
only a short while after the previous planning was complete.</li>
  <li>Planning that is too frequent, where a team doesn’t even have a chance to
deliver the previous OKRs.</li>
</ul>

<p>To mitigate those, I suggest:</p>

<ul>
  <li>OKR planning no more than once a quarter. Every six months would be a better
cadence, if it’s possibly to plan that far ahead.</li>
  <li>OKR planning process only last 2 weeks: this forces a lot of thinking and
collaboration, but leaves 10 weeks or more to focus on execution.</li>
</ul>

<h3 id="concern-things-change-too-quickly-to-write-good-okrs">Concern: things change too quickly to write good OKRs</h3>

<p>This is a valid criticism. OKRs are built around long-term planning, and
sometimes that isn’t always possible. If a KR is shifting constantly, it may be
a sign of, frankly, some organizational dysfunction that any goal setting
exercise cannot address.</p>

<p>However, this is also often a symptom of the OKRs themselves: perhaps the OKRs
have over-specified and implementation detail, rather than serve as a measure of
the achievement of a goal.</p>

<p>Taking improving the performance of a C++ codebase as an example. An
overspecified KR may look like:</p>

<ul>
  <li>update clangd and achieve a 20% latency reduction in web service latency.</li>
</ul>

<p>The “update clangd” detail is superfluous and overspecified: the goal is to make
things faster, and if you find some better way to do that after a couple weeks
of investigation, you should be able to switch the approach whenever to achieve
that.</p>

<h3 id="concerns-the-scope-of-the-krs-is-too-constrained">Concerns: the scope of the KRs is too constrained</h3>

<p>Sometimes a KR can feel like it’s constraining you to a specific solution. I
have found that this, similar to a KR that goes stale quickly, is often
caused by being too tightly coupled to a specific solution.</p>

<p>A solution may change mid-quarter, and the KRs should be authored so that as
long as the outcome is achieved, it can be considered complete.</p>

<h2 id="examples-of-bad-objectives">Examples of bad objectives</h2>

<ul>
  <li>not framed in terms of a business goal:
    <ul>
      <li>bad: “oncall rotation”</li>
      <li>good: “achieve high availability of P0 services”</li>
    </ul>
  </li>
</ul>

<h2 id="examples-of-bad-krs">Examples of bad KRs</h2>

<ul>
  <li>overspecified: A KR that includes precise implementation details.
    <ul>
      <li>bad: “achieve a 20% latency improvement by adding flash attention support”</li>
      <li>good” “achieve a 20% latency improvement in model training”</li>
    </ul>
  </li>
  <li>underspecified: A KR that doesn’t include a specific target.
    <ul>
      <li>bad: “ship fewer bugs”</li>
      <li>good: “20% fewer P0 issues discovered in production” or “3 or less P0 incidents in production”</li>
    </ul>
  </li>
  <li>unmeasurable: A KR that is not quantifiable.
    <ul>
      <li>bad: “users are happy with their CI execution”</li>
      <li>good: “p99 for CI test execution is within 2 minutes”</li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="okrs" /><summary type="html"><![CDATA[OKRs and goal setting]]></summary></entry><entry><title type="html">Finding a low latency game controller</title><link href="https://y.tsutsumi.io/low-latency-gaming-controller/" rel="alternate" type="text/html" title="Finding a low latency game controller" /><published>2025-06-27T07:00:00+00:00</published><updated>2025-06-27T07:00:00+00:00</updated><id>https://y.tsutsumi.io/low-latency-controllers</id><content type="html" xml:base="https://y.tsutsumi.io/low-latency-gaming-controller/"><![CDATA[<h1 id="finding-a-low-latency-game-controller">Finding a low latency game controller</h1>

<p>I’ve been playing some <a href="https://www.expedition33.com/">Clair Obscur: Expedition
33</a>, and it’s an amazing game.</p>

<p>However, one of the major issues I run into is with timing the dodges: during
battles on the hardest difficulty, if you don’t dodge, your party dies very quickly:</p>

<p><img src="./2025-06-27-low-latency-controller-exp33.png" alt="image" /></p>

<p>Dodging comes down to timing pressing a button right when the enemy attacks.
It isn’t easy, but I noticed that I was having a really hard
time with it. That’s when I realized that it was due to my controller (the <a href="https://www.8bitdo.com/lite2/">8bitdo Lite 2</a>).</p>

<h2 id="what-affects-controller-latency">What affects controller latency?</h2>

<p>There are two main factors to consider:</p>

<ul>
  <li>The <em>polling rate</em> of the controller: how frequently it checks to see if a
button or stick is pressed. This can introduce a maximum delay of <code class="language-plaintext highlighter-rouge">1000ms /
freq_hz</code>.</li>
  <li>The <em>connection latency</em> of the controller: the amount of time it takes for a
signal to reach the device. For bluetooth this can be highly variable, while
wired and 2.4Ghz connections can be much more stable.</li>
</ul>

<p>In my case, there is a third factor, since I’m streaming my setup via
<a href="https://moonlight-stream.org/">moonlight</a>: the latency between my device and
the device running the game. On a 5Ghz wireless connection in my house, that’s
3-4 milliseconds.</p>

<h2 id="8bitdo2-lite-2-latency">8bitdo2 Lite 2 latency</h2>

<p>So what was the worst-case latency of the 8bitdo Lite 2? We can reference <a href="https://www.youtube.com/watch?v=O6cZIfD6uLw">this
youtube review</a>, which has an
excellent test.</p>

<p>The bottom line looks like:</p>

<ul>
  <li>Average polling rate 44hz, so 22 milliseconds!</li>
</ul>

<p>The review didn’t mention bluetooth, so I’m looking for a way to test that. But
assuming at least an additional 4-5 milliseconds of latency, that’s 25
milliseconds it’s adding to those button presses!</p>

<h2 id="what-is-the-best-low-latency-controller">What is the best low latency controller?</h2>

<p>In short, look at <a href="https://gamepadla.com/">gamepedla</a>, which has timings for
many controllers.</p>

<p>But I chose the <a href="https://www.8bitdo.com/ultimate-2-wireless-controller/">8bitdo Ultimate
2</a>. At a 1000hz polling
rate and measured sub 5ms latency for both wired and via 2.4Ghz, It’ll ensure a
lag-free experience if needed. In addition, the 10ms bluetooth latency isn’t too
bad!</p>

<p>While I’m waiting for that, I had an Xbox Core Controller lying around, and the
latency there <a href="https://gamepadla.com/xbox-core-controller.html">isn’t bad at
all</a>. Even on bluetooth it’s
11-12ms, comparable to bluetooth on the 8bitdo Ultimate 2.</p>]]></content><author><name></name></author><category term="gaming" /><summary type="html"><![CDATA[Finding a low latency game controller]]></summary></entry><entry><title type="html">My tips on debugging</title><link href="https://y.tsutsumi.io/debugging" rel="alternate" type="text/html" title="My tips on debugging" /><published>2025-06-07T07:00:00+00:00</published><updated>2025-06-07T07:00:00+00:00</updated><id>https://y.tsutsumi.io/debugging</id><content type="html" xml:base="https://y.tsutsumi.io/debugging"><![CDATA[<h1 id="my-tips-on-debugging">My tips on debugging</h1>

<p>Updated: 2025-06-07</p>

<p>This is a running collection of my thoughts on debugging. I’ll update these over time.</p>

<h2 id="improve-errors-minimize-print-statements">Improve errors, minimize print statements</h2>

<p>One common type of debugging is “print” style - just print the data you need at the time. Often this occurs because you don’t have enough information in the error itself:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"conflicting args"</span><span class="p">)</span>
</code></pre></div></div>

<p>Rather than add a print statement temporarily, just make the error better - it helps the next person when they come along:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s">"conflict args. Args found: </span><span class="si">{</span><span class="n">args</span><span class="si">}</span><span class="s">)
</span></code></pre></div></div>]]></content><author><name></name></author><category term="coding" /><summary type="html"><![CDATA[My tips on debugging]]></summary></entry><entry><title type="html">Result, Situation, Action: a new take on the STAR format</title><link href="https://y.tsutsumi.io/rsa" rel="alternate" type="text/html" title="Result, Situation, Action: a new take on the STAR format" /><published>2025-02-08T07:00:00+00:00</published><updated>2025-02-08T07:00:00+00:00</updated><id>https://y.tsutsumi.io/result-situation-action</id><content type="html" xml:base="https://y.tsutsumi.io/rsa"><![CDATA[<h1 id="result-situation-action">Result, Situation, Action</h1>

<p>The <a href="https://en.wikipedia.org/wiki/Situation,_task,_action,_result">STAR</a> format
of answering an interview question is very popular. But in my experience a twist
on that ends up being more effective: result, situation, then action.</p>

<p>The STAR format kind of buries the lead - it forces the interviewer into a
long-winded story and they’re not engaged because they don’t understand the
value of it. By giving them some idea of the impact that your story had (e.g. I
saved the company $XXX dollars, we reduced developer wait times by 20 minutes on
a 1-hour task), that gives the interviewer a reason to listen.</p>

<p>In addition, I find the “task” part of the story tends to be extraneous - when
you only have 30 minutes to an hour with the interviewer <em>total</em>, you need to
shave the minutes where it matters.</p>

<p>Although the whole “my task was X” may take 1 minute to explain, it’s better to just explain what you <em>did</em>.</p>

<h2 id="example">Example</h2>

<p>Result: I increased revenue for the company by XX percent.</p>

<p>Situation: In multiple customer meetings, I noticed that in the follow-up of
customers meetings, they would always mention having trouble onboarding onto
feature X.</p>

<p>Action: I wrote up a user guide, and we added a link the first time a customer
accessed the feature, as well as included it in the onboarding guide.</p>

<h2 id="summary">Summary</h2>

<p>I call this “RSA” pronounced “rizza”. Try it out!</p>]]></content><author><name></name></author><category term="coding" /><summary type="html"><![CDATA[Result, Situation, Action]]></summary></entry></feed>