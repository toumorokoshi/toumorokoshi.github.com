<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://y.tsutsumi.io/feed/index.xml" rel="self" type="application/atom+xml" /><link href="https://y.tsutsumi.io/" rel="alternate" type="text/html" /><updated>2026-01-04T00:05:53+00:00</updated><id>https://y.tsutsumi.io/feed/index.xml</id><title type="html">Yusuke Tsutsumi</title><subtitle>My blog on software, productivity, and obsessively optimizing. I work at Google, ex-Zillow. Thoughts my own.</subtitle><entry><title type="html">Rebuilding AlexNet</title><link href="https://y.tsutsumi.io/alexnet" rel="alternate" type="text/html" title="Rebuilding AlexNet" /><published>2025-12-26T07:00:00+00:00</published><updated>2025-12-26T07:00:00+00:00</updated><id>https://y.tsutsumi.io/rebuilding-alexnet</id><content type="html" xml:base="https://y.tsutsumi.io/alexnet"><![CDATA[<h1 id="rebuilding-alexnet">Rebuilding AlexNet</h1>

<p>My role at GM (aka ex-Cruise) has started pulling me and more and more into the machine learning space, in particular models that are critical to Autonomous vehicles such as perception, planning, world models, etc. For context, I have traditionally worked more on the system performance / vehicle software side, not the models directly.</p>

<p>As such, I’ve started to try to get a better grasp of ML fundamentals. One of my first forays into getting hand-on experience is rebuilding <a href="https://en.wikipedia.org/wiki/AlexNet">AlexNet</a>, one of the most influential models to modern machine learning. It was the first model to get a top-5 error rate of 15.3% in the ImageNet Large Scale Recognition Challenge and using convolutional neural networks in the process.</p>

<p>This post serves to document some of my learnings as I attempted to recreate it.</p>

<h1 id="my-general-approach">My General Approach</h1>

<p>I didn’t start by recreating AlexNet immediately: I instead started with
a simple neuralnet + RELU based model, to try to understand the individual impact of various model architectures.</p>

<p>I also tried to use vanilla PyTorch, although I did build a <a href="https://github.com/toumorokoshi/yft-ml-sandbox/tree/main/alexnet">bazel-based build workflow</a> since I prefer the hermicity and relatively simple dependency management of multiple languages.</p>

<p>To keep the training fast (since I was focused more experimentation rather than production use cases), I used the much smaller <a href="https://github.com/fastai/imagenette">imagenette</a> dataset rather than ImageNet which AlexNet was trained and tested against. This has only 10 classifications, as well as less data.</p>

<h1 id="the-learnings">The Learnings</h1>

<h2 id="terminology">Terminology</h2>

<p>If I had one big takeaway, it was understanding the terminology a lot more deeply! Knowing the terms logit, epoch, stochastic gradient descent, maxpool, generalization, and overfitting in a more intuitive and visceral helps me understand ideas discussed within our AV organization much more quickly.</p>

<h2 id="always-randomize-datasets">Always randomize datasets</h2>

<p>result: 9.99% -&gt; 40%</p>

<p>My initial iteration had a 9.99% pass rate, after adding the debugger and looking at the predicted output of the model by printing the final logit layer, I noticed that the output was always a specific classification, no matter what the input was.</p>

<p>This was caused by me not randomizing the dataset - effectively every epoch, the model would get exclusively trained against one classification for the last 10% of the time, biasing it toward that one classification.</p>

<p>After I fixed that, my simple 2-layer neural network got up to 40% accuracy!</p>

<h2 id="misc-things-that-helped-the-neural-net">Misc things that helped the neural net</h2>

<p>The highest I could get the neural network was 44%. Here’s a table of things I tried:</p>

<table>
  <thead>
    <tr>
      <th>Accuracy</th>
      <th>Epochs</th>
      <th>Batch Size</th>
      <th>Learning Rate</th>
      <th>Shuffled</th>
      <th>Momentum</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>44.6%</td>
      <td>10</td>
      <td>32</td>
      <td>0.01</td>
      <td>yes</td>
      <td>0</td>
    </tr>
    <tr>
      <td>42.9%</td>
      <td>20</td>
      <td>16</td>
      <td>0.01</td>
      <td>yes</td>
      <td>0</td>
    </tr>
    <tr>
      <td>41.9%</td>
      <td>10</td>
      <td>16</td>
      <td>0.001</td>
      <td>yes</td>
      <td>0</td>
    </tr>
    <tr>
      <td>40.9%</td>
      <td>10</td>
      <td>32</td>
      <td>0.001</td>
      <td>yes</td>
      <td>0</td>
    </tr>
    <tr>
      <td>38.0%</td>
      <td>5</td>
      <td>16</td>
      <td>0.001</td>
      <td>yes</td>
      <td>0</td>
    </tr>
    <tr>
      <td>11%</td>
      <td>10</td>
      <td>16</td>
      <td>0.01</td>
      <td>no</td>
      <td>0.9</td>
    </tr>
    <tr>
      <td>9.9%</td>
      <td>10</td>
      <td>16</td>
      <td>0.001</td>
      <td>no</td>
      <td>0</td>
    </tr>
  </tbody>
</table>

<p>I could tweak batch size and learning rate to help it converge more quickly.</p>

<h2 id="single-layer-convolution">Single-layer convolution</h2>

<p>Switching to even a single-layer of convolution immediately caused the model to jump to 55%: as explained in the AlexNet paper, I think this can primarily be attributed to better generalization of the learnings in one particular segment of the image: without convolution and some form of feature consolidation, it’s really impossible to divine general relationships that shift across the matrix like the position of a dog in the image.</p>

<h2 id="60-epochs-is-the-good-range">60+ epochs is the good range.</h2>

<p>For most of the convolution models, many had some continued gain up to about 45 to 60 epochs. I believe this will be highly dependent on how much training data is available, but it’s interesting to thing that running the model against the same data 40+ times is required to hit some sort of ceiling.</p>

<h2 id="dropout-added-10">dropout added 10%</h2>

<p>The original AlexNet paper suggests that introducing dropout in the final connected layer (2 layers of linear + ReLU combinations) results in a significant raining improvement. They theorize this is due to the individual “neurons” in the matrices start to embed more generalization into themselves, not requiring the constant presence of the upstream neurons.</p>

<p>In my experience, this increased the accuracy by roughly 10% from the non-dropout version. So clearly, the neurons embedding more information is a critical part to better models.</p>

<h2 id="flipping-image-data-added-10-accuracy">flipping image data added 10% accuracy</h2>

<p>There isn’t much more I could do with model architecture to increase the accuracy, but I got some pointers to go look at data.</p>

<p>The biggest contributor was just flipping the image data half the time: I think this effectively doubles the dataset to some extent. Which implies that training data might be a bottleneck now rather than just learning capacity.</p>

<p>Some other things I tried, which has less of an effect:</p>

<ul>
  <li>Scaling the data up and cropping didn’t seem to improve much. Maybe 1-2% at most.</li>
  <li>Changing exposure, brightness also had maybe a 1-2% difference, although training still ended up in the same range.</li>
</ul>

<h2 id="my-current-score-78">my current score: 78%</h2>

<p>For the imagenette, I was able to get a 78% score after 60 epochs. This is pretty good, as the original AlexNet paper, with a much larger dataset and longer training time, reached 85%.</p>

<p>I did not see any overfitting directly (which I’d expect to see via loss curve that has an upward trend after multiple epochs). But I did see the loss plateau after about 45 epochs or so.</p>

<p>Some quick notes:</p>

<ul>
  <li>Adding RMSNorm in the first 2 layers added roughly 1-2%.</li>
  <li>Originally I had only convolutional layers with stride=2 rather than maxpool. With the maxpool layers instead I had a 4-5% bump.</li>
</ul>]]></content><author><name></name></author><category term="coding" /><summary type="html"><![CDATA[Rebuilding AlexNet]]></summary></entry><entry><title type="html">How I code with AI</title><link href="https://y.tsutsumi.io/coding-with-ai" rel="alternate" type="text/html" title="How I code with AI" /><published>2025-11-08T07:00:00+00:00</published><updated>2025-11-08T07:00:00+00:00</updated><id>https://y.tsutsumi.io/my-ai-best-practices</id><content type="html" xml:base="https://y.tsutsumi.io/coding-with-ai"><![CDATA[<h1 id="how-i-code-with-ai">How I code with AI</h1>

<p>Recently, like many developers, I’ve been experimenting a lot with coding with AI.</p>

<p>Overall I’ve found that I am more productive with AI: some PRs are vastly easier, although I do write a decent bit myself (especially with https://aep.dev/, which is basically an english specification).</p>

<p>I’ll update this page over time as I learn some new tricks as well.</p>

<h2 id="adding-a-personal-coding-styleguide">Adding a personal coding styleguide</h2>

<p>Most editors provide a way to add a custom styleguide. vscode calls these “instructions”.</p>

<p>My personal instructions are here:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
applyTo: '**'
---
# yft's personal coding guidelines
- prefer fewer comments. Only add them when the code is not self-explanatory.
- prefer functional programming.
- IO with network and filesystem should be in wrapper functions that call inner functions that work on data structures.
- unit tests should be written to run on the data structures directly rather than through IO, with only a single test on IO to serve as an integration test.
- prefer composition over inheritance.
- prefer to use constants, especially for hard-coded directories or values.
- helper functions should have minimal logic.
</code></pre></div></div>

<h2 id="one-off-experiences">One-off experiences</h2>

<h3 id="2025-11-10-using-jules-and-github-copilot">2025-11-10: using Jules and GitHub copilot</h3>

<p>I had trying Jules (https://jules.google/docs/) this weekend on a personal project. It’s kind of similar to this so thought it was worth bringing up.</p>

<p>Basically I had it write a PR to create a new proto schema for me: https://github.com/aep-dev/aep-components/pull/41. The workflow is you add a label “jules” to a GitHub issue, and it goes and generates a PR.</p>

<p>The first pass (like many AI tools) was ok, but had a lot of changes that needed. Many were me not being clear about the initial expectations. But one major hangup was that jules didn’t understand that there was a github CI workflow that it should have run to find more errors. I tried to explain it here, but gave up because it was still failing and I was moving toward comments that just hand-fixed it: https://github.com/aep-dev/aep-components/pull/41#issuecomment-3506923393</p>

<p>I actually tried the exact same change with the built-in copilot generator, which was much closer: https://github.com/aep-dev/aep-components/pull/44. It seemed to better understand the linting and ci workflows, and didn’t have the same breakages with buf.</p>

<p>My workflow was still leaving a bunch of comments, many of which are direct code fixes. But I was able to massage and merge it in.</p>

<p>Ultimately I think this would have been faster for me to have just done locally with cursor / copilot, then hand-fix and submit the PR myself, then have a PR generator do it. But it felt like needing a fundamental understanding of the tooling in the repo was critical.</p>]]></content><author><name></name></author><category term="programming" /><summary type="html"><![CDATA[How I code with AI]]></summary></entry><entry><title type="html">The Anxious Generation by Jonathan Haidt</title><link href="https://y.tsutsumi.io/books/anxious-generation" rel="alternate" type="text/html" title="The Anxious Generation by Jonathan Haidt" /><published>2025-11-02T07:00:00+00:00</published><updated>2025-11-02T07:00:00+00:00</updated><id>https://y.tsutsumi.io/books/the-anxious-generation</id><content type="html" xml:base="https://y.tsutsumi.io/books/anxious-generation"><![CDATA[<h1 id="the-anxious-generation">The Anxious Generation</h1>

<p>I really liked reading the book “The Anxious generation” by Jonathan Haidt.</p>

<p>The book argues for less screen time for children, especially social media at
younger ages, which can be harmful.</p>

<p>The jist of the book is:</p>

<p>1: The time your child spends outside or interacting with kids is more valuable that almost all
forms of screen time. I like this argument because it’s a relative comparison, rather than saying
that screen time is inherently bad (which, as someone who spend his whole childhood in front a
screen and turned out… not terrible, tend to disagree).</p>

<p>2: Social media for young children can be a terrible infleunce. There’s cyberbullying, which can be
much worse than regular bullying as people in general are willing to be a lot more horrible online.
In addition, there’s the implicit pressure to live and look as glamorous as these famous influencers
online, resulting in body dysmoprhia.</p>

<p>Although the book references some studies, I agree on an anecdotal level that social media is not a
necessary part of life for adolescents, and that screentime is something with, depending on the
activity, rapidly diminishing returns, and a balance with other activities in life is critical.</p>]]></content><author><name></name></author><category term="book-reports" /><summary type="html"><![CDATA[The Anxious Generation]]></summary></entry><entry><title type="html">Setting up a streaming gaming Ubuntu PC for sunshine/moonlight</title><link href="https://y.tsutsumi.io/ubuntu-sunshine/" rel="alternate" type="text/html" title="Setting up a streaming gaming Ubuntu PC for sunshine/moonlight" /><published>2025-07-18T07:00:00+00:00</published><updated>2025-07-18T07:00:00+00:00</updated><id>https://y.tsutsumi.io/setting-up-moonlight-ubuntu</id><content type="html" xml:base="https://y.tsutsumi.io/ubuntu-sunshine/"><![CDATA[<h1 id="setting-up-a-streaming-gaming-ubuntu-pc-for-moonlight">Setting up a streaming gaming Ubuntu PC for moonlight</h1>

<p>I’ve been working on building a new streaming gaming PC (that ordeal I will save
for a future blog post), but in the process I had some difficulties with getting
moonlight set up just the way I like it. So I figured these might be helpful to
others.</p>

<p>My goal is to have a PC that I use exclusively for streaming games via
<a href="https://app.lizardbyte.dev/Sunshine/?lng=en-US">sunshine</a>. Sunshine is the
replacement for Nvidia game streaming that was deprecated, and work as a
complement to the streaming client <a href="https://moonlight-stream.org/">moonlight</a>.
It allows for the streaming of video and audio to a client, which in turn
streams controls (keyboard and / or a controller) back to the server.</p>

<p>When all is said and done, I want what is effectively a box in my closet with
only two things attached: a power cable, and an ethernet cable (for a
low-latency wired connection to my network). I do not want dangling mice,
keyboards or monitors. Just a box that connects to the internet, and is
available for me to stream from.</p>

<p>To accomplish this, I needed a few things:</p>

<ol>
  <li>A way to have a virtual display even if a physical monitor isn’t plugged in.</li>
  <li>A way to login as a user right away (since sunshine needs a valid user).</li>
  <li>A way to have sunshine available when the machine boots.</li>
  <li>A way to wake the machine after it suspends</li>
</ol>

<h2 id="attaching-a-virtual-display">Attaching a virtual display</h2>

<p>This was relatively easy!</p>

<p>You can buy a <a href="https://www.google.com/search?q=dummy+plug+hdmi">dummy plug</a> that
you just plug into the GPU. When you’ve got your PC set up, just replace your
actual monitor with that.</p>

<h2 id="logging-in-right-away">Logging in right away</h2>

<p>This one just requires careful setup:</p>

<ol>
  <li>setup autologin in your operating system (varies by distro).</li>
  <li><em>do not</em> use disk encryption.</li>
</ol>

<p>(2) is the major gotcha: if you add disk encryption it’ll ask for your password
every time your computer starts up, which means plugging in a keyboard
every time. you could just leave a keyboard by your computer for this purpose,
but I’ve found I do have to restart my computer from time to time, and making
bringing it back up as easy as pressing a button is a nice usability improvement.</p>

<h2 id="start-sunshine-when-the-machine-boots">Start sunshine when the machine boots</h2>

<p>This was pretty hard for me. Normally, this is a matter of installing the
sunshine .deb from the <a href="https://github.com/LizardByte/Sunshine/releases">github
releases</a>, but in my case I had
to use Ubuntu 25.05 for a newer kernel version that supports my AMD GPU. Only
LTS release are supported for sunshine.</p>

<p>For a non-lts release, I found that using the <code class="language-plaintext highlighter-rouge">.appimage</code> worked the best. This
unfortunately didn’t work for me without root, so I had to run sunshine as sudo:</p>

<p><code class="language-plaintext highlighter-rouge">sudo -i PULSE_SERVER=unix:/run/user/$(id -u $whoami)/pulse/native /usr/bin/sunshine</code></p>

<p>(I moved the appimage to /usr/bin/sunshine).</p>

<p>I put this in a script, which I then added as a <a href="https://help.ubuntu.com/stable/ubuntu-help/startup-applications.html.en">startup application</a>:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/usr/bin/env bash</span>
<span class="c"># I had to add a sleep to let the graphical interface completely come up.</span>
<span class="c"># otherwise I would see a black screen.</span>
<span class="nb">sleep </span>10
<span class="nb">exec sudo</span> <span class="nt">-i</span> <span class="nv">PULSE_SERVER</span><span class="o">=</span>unix:/run/user/<span class="si">$(</span><span class="nb">id</span> <span class="nt">-u</span> <span class="nv">$whoami</span><span class="si">)</span>/pulse/native /usr/bin/sunshine
</code></pre></div></div>

<p>This works, but still has some wonkiness:</p>

<ul>
  <li>steam can’t be launched via moonlight because it tries to start it as root.</li>
</ul>

<h2 id="a-way-to-wake-the-machine-after-it-suspends">A way to wake the machine, after it suspends</h2>

<p>Now you probably don’t want your machine on all the time - instead, you’d want
it to suspend after a reasonable idle time frame, and have the ability to turn
it one when you need it.</p>

<p>I tried a few approach, none of which worked or were reliable (I think something
else was resetting my network configuration):</p>

<ul>
  <li><a href="https://wiki.archlinux.org/title/Wake-on-LAN">adding a rule to /systemd/network</a></li>
  <li>Adding a vanilla systemd unit to run ethtool.</li>
</ul>

<p>Ultimately I landed on the following:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># /lib/systemd/system/wol@.service
[Unit]
Description=Wake-on-lan for %i
# This means run after the network is online, as well
# as after suspension.
After=network-online.target suspend.target

[Service]
Type=oneshot
# something else in my OP keeps setting the WOL settings on the device.
# adding this sleep gave whatever time to configure it's settings, so I could override them.
ExecStartPre=/usr/bin/sleep 10
ExecStart=/sbin/ethtool -s %i wol g

[Install]
# if we run network-online and suspend.target, run this too.
WantedBy=network-online.target suspend.target
</code></pre></div></div>

<p>Installed by:</p>

<p><code class="language-plaintext highlighter-rouge">systemctl enable wol@${ethid}</code></p>

<p>Where <code class="language-plaintext highlighter-rouge">${ethid}</code> is the interface id you can get from <code class="language-plaintext highlighter-rouge">nmcli</code>.</p>

<h2 id="conclusion">Conclusion</h2>

<p>And that’s it! Most of it was troubleshooting systemd units. My takeaways are:</p>

<ul>
  <li>try to use systemd units as much as possible. It’s an extremely powerful
system and enables you to start your process with the right sequencing and situations.</li>
  <li>try to use LTS releases of Ubuntu if at all possible. Moonlight installation
has been tricky, and I’m sure other packages only support LTS. I’m looking
forward to upgrading to 2026.04 precisely for this benefit.</li>
</ul>

<h3 id="why-linux-why-ubuntu">Why linux? Why ubuntu?</h3>

<p>Although not for everyone, I like to use Linux for a few reasons:</p>

<ol>
  <li>Linux distributions don’t to forced updates, thereby allowing gaming
uninterrupted by some malware update.</li>
  <li>Windows has a ton of bloatware and ads.</li>
  <li>Linux is “free” - it doesn’t cost a cent to run it, and distros often work
for years.</li>
  <li>With the work done on <a href="https://www.winehq.org/">wine</a> and
<a href="https://github.com/ValveSoftware/Proton">proton</a>, Windows games run
extremely smooth on Linux now.</li>
</ol>

<p>For Ubuntu, I would go with <a href="https://bazzite.gg/">bazzite</a> for a pure gaming PC,
but as this computer will be the most powerful one I have, I wanted it to have
the option to be more general purpose in case I get the itch to do some real
software development on it (maybe ML or some CPU emulation project).</p>]]></content><author><name></name></author><category term="computers" /><summary type="html"><![CDATA[Setting up a streaming gaming Ubuntu PC for moonlight]]></summary></entry><entry><title type="html">OKRs and goal setting</title><link href="https://y.tsutsumi.io/okrs/" rel="alternate" type="text/html" title="OKRs and goal setting" /><published>2025-07-11T07:00:00+00:00</published><updated>2025-07-11T07:00:00+00:00</updated><id>https://y.tsutsumi.io/okrs-and-goal-setting</id><content type="html" xml:base="https://y.tsutsumi.io/okrs/"><![CDATA[<h1 id="okrs-and-goal-setting">OKRs and goal setting</h1>

<p>I’ve had a lot of conversations around OKRs, so I wanted to write down my
thoughts around them.</p>

<h2 id="what-are-okrs-and-how-are-they-defined">What are OKRs, and how are they defined</h2>

<p>OKRs stand for “objectives” and “key results”.</p>

<p>OKRs were developed by Andy Grove at Intel in the 1970s and later popularized by
John Doerr, who learned about them while working at Intel, and then introduced
them to Google in 1999. The framework has since been adopted by many technology
companies and organizations worldwide.</p>

<p><strong>Objectives</strong> are qualitative, inspirational goals that answer the question “What do we want to achieve?” They should be:</p>
<ul>
  <li>Clear and specific</li>
  <li>Time-bound (typically quarterly)</li>
  <li>Ambitious but achievable</li>
  <li>
    <h2 id="aligned-with-company-strategy">Aligned with company strategy</h2>
    <p>Objectives create alignment across teams and drive focus on the most important
priorities for the organization.</p>
  </li>
</ul>

<p><strong>Key Results</strong> are quantitative, measurable outcomes that answer the question “How will we know if we achieved our objective?” They should be:</p>
<ul>
  <li>Specific and measurable</li>
  <li>Time-bound</li>
  <li>Aggressive but realistic</li>
  <li>Leading indicators of success</li>
</ul>

<p>Key results provide a quantitative measure of an objective: if all of the KRs
are reached, the objective is considered accomplished.</p>

<h2 id="hierarchy-of-okrs">Hierarchy of OKRs</h2>

<pre><code class="language-mermaid">graph TD
    O[Objective: Improve Customer Satisfaction]
    KR[Key Result: Achieve 90% Customer Satisfaction Score]
    P[Project: Implement Customer Feedback System]

    P --&gt; KR --&gt; O
</code></pre>

<p>This hierarchy shows how:</p>
<ul>
  <li><strong>Objectives</strong> (top level) define what you want to achieve</li>
  <li><strong>Key Results</strong> (middle level) measure progress toward objectives</li>
  <li><strong>Projects</strong> (bottom level) are the actual work initiatives that drive key results</li>
</ul>

<h2 id="addressing-okr-concerns">Addressing OKR Concerns</h2>

<p>I’ve often heard from many ICs and technical managers that OKRs feel like a
pedantic exercise that only serves as busywork, and doesn’t provide real value
to the organization, or might constrain the scope of the work unnecessarily. I
hope to argue that, with the right process, OKRs can be neither.</p>

<h3 id="okrs-create-a-common-language-of-communication">OKRs create a common language of communication</h3>

<p>In my experience working with senior leadership teams, the biggest challenge in
communication comes from <em>not articulating engineering projects in terms of a
measure that leadership can truly appreciate</em>. This requires a mindset shift,
especially for engineers who are used to dealing with problems that require deep
context to solve and fully appreciate.</p>

<p>Senior leaders often have dozens, if not hundreds of members in their
organization. They will likely not have the bandwidth to intimately understand
the work you’re doing, although they probably want to. This puts an individual
engineer or a team at a risk: if there is a disconnect in context and a common
language across the two levels, the engineer may be working on amazing things
that go unrecognized, and the leader may miss the opportunity to give critical
feedback on the alignment of goals. OKRs serve as that missing common language,
helping frame the technical work being done in terms of the impact to the
organization and a quantitative way to measure it.</p>

<h3 id="concern-okrs-are-too-heavyweight">Concern: OKRs are too heavyweight</h3>

<p>When there is feedback that the OKR process is busywork, I think a lot of that
comes from a heavyweight OKRs process. Including:</p>

<ul>
  <li>A process that forces very stringent requirements and arguing pedantries, like
requiring specific phrasing or formatting.</li>
  <li>A process that stretches weeks at a time, where the next OKR planning begins
only a short while after the previous planning was complete.</li>
  <li>Planning that is too frequent, where a team doesn’t even have a chance to
deliver the previous OKRs.</li>
</ul>

<p>To mitigate those, I suggest:</p>

<ul>
  <li>OKR planning no more than once a quarter. Every six months would be a better
cadence, if it’s possibly to plan that far ahead.</li>
  <li>OKR planning process only last 2 weeks: this forces a lot of thinking and
collaboration, but leaves 10 weeks or more to focus on execution.</li>
</ul>

<h3 id="concern-things-change-too-quickly-to-write-good-okrs">Concern: things change too quickly to write good OKRs</h3>

<p>This is a valid criticism. OKRs are built around long-term planning, and
sometimes that isn’t always possible. If a KR is shifting constantly, it may be
a sign of, frankly, some organizational dysfunction that any goal setting
exercise cannot address.</p>

<p>However, this is also often a symptom of the OKRs themselves: perhaps the OKRs
have over-specified and implementation detail, rather than serve as a measure of
the achievement of a goal.</p>

<p>Taking improving the performance of a C++ codebase as an example. An
overspecified KR may look like:</p>

<ul>
  <li>update clangd and achieve a 20% latency reduction in web service latency.</li>
</ul>

<p>The “update clangd” detail is superfluous and overspecified: the goal is to make
things faster, and if you find some better way to do that after a couple weeks
of investigation, you should be able to switch the approach whenever to achieve
that.</p>

<h3 id="concerns-the-scope-of-the-krs-is-too-constrained">Concerns: the scope of the KRs is too constrained</h3>

<p>Sometimes a KR can feel like it’s constraining you to a specific solution. I
have found that this, similar to a KR that goes stale quickly, is often
caused by being too tightly coupled to a specific solution.</p>

<p>A solution may change mid-quarter, and the KRs should be authored so that as
long as the outcome is achieved, it can be considered complete.</p>

<h2 id="examples-of-bad-objectives">Examples of bad objectives</h2>

<ul>
  <li>not framed in terms of a business goal:
    <ul>
      <li>bad: “oncall rotation”</li>
      <li>good: “achieve high availability of P0 services”</li>
    </ul>
  </li>
</ul>

<h2 id="examples-of-bad-krs">Examples of bad KRs</h2>

<ul>
  <li>overspecified: A KR that includes precise implementation details.
    <ul>
      <li>bad: “achieve a 20% latency improvement by adding flash attention support”</li>
      <li>good” “achieve a 20% latency improvement in model training”</li>
    </ul>
  </li>
  <li>underspecified: A KR that doesn’t include a specific target.
    <ul>
      <li>bad: “ship fewer bugs”</li>
      <li>good: “20% fewer P0 issues discovered in production” or “3 or less P0 incidents in production”</li>
    </ul>
  </li>
  <li>unmeasurable: A KR that is not quantifiable.
    <ul>
      <li>bad: “users are happy with their CI execution”</li>
      <li>good: “p99 for CI test execution is within 2 minutes”</li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="okrs" /><summary type="html"><![CDATA[OKRs and goal setting]]></summary></entry><entry><title type="html">Finding a low latency game controller</title><link href="https://y.tsutsumi.io/low-latency-gaming-controller/" rel="alternate" type="text/html" title="Finding a low latency game controller" /><published>2025-06-27T07:00:00+00:00</published><updated>2025-06-27T07:00:00+00:00</updated><id>https://y.tsutsumi.io/low-latency-controllers</id><content type="html" xml:base="https://y.tsutsumi.io/low-latency-gaming-controller/"><![CDATA[<h1 id="finding-a-low-latency-game-controller">Finding a low latency game controller</h1>

<p>I’ve been playing some <a href="https://www.expedition33.com/">Clair Obscur: Expedition
33</a>, and it’s an amazing game.</p>

<p>However, one of the major issues I run into is with timing the dodges: during
battles on the hardest difficulty, if you don’t dodge, your party dies very quickly:</p>

<p><img src="2025-06-27-low-latency-controller-exp33.png" alt="image" /></p>

<p>Dodging comes down to timing pressing a button right when the enemy attacks.
It isn’t easy, but I noticed that I was having a really hard
time with it. That’s when I realized that it was due to my controller (the <a href="https://www.8bitdo.com/lite2/">8bitdo Lite 2</a>).</p>

<h2 id="what-affects-controller-latency">What affects controller latency?</h2>

<p>There are two main factors to consider:</p>

<ul>
  <li>The <em>polling rate</em> of the controller: how frequently it checks to see if a
button or stick is pressed. This can introduce a maximum delay of <code class="language-plaintext highlighter-rouge">1000ms /
freq_hz</code>.</li>
  <li>The <em>connection latency</em> of the controller: the amount of time it takes for a
signal to reach the device. For bluetooth this can be highly variable, while
wired and 2.4Ghz connections can be much more stable.</li>
</ul>

<p>In my case, there is a third factor, since I’m streaming my setup via
<a href="https://moonlight-stream.org/">moonlight</a>: the latency between my device and
the device running the game. On a 5Ghz wireless connection in my house, that’s
3-4 milliseconds.</p>

<h2 id="8bitdo2-lite-2-latency">8bitdo2 Lite 2 latency</h2>

<p>So what was the worst-case latency of the 8bitdo Lite 2? We can reference <a href="https://www.youtube.com/watch?v=O6cZIfD6uLw">this
youtube review</a>, which has an
excellent test.</p>

<p>The bottom line looks like:</p>

<ul>
  <li>Average polling rate 44hz, so 22 milliseconds!</li>
</ul>

<p>The review didn’t mention bluetooth, so I’m looking for a way to test that. But
assuming at least an additional 4-5 milliseconds of latency, that’s 25
milliseconds it’s adding to those button presses!</p>

<h2 id="what-is-the-best-low-latency-controller">What is the best low latency controller?</h2>

<p>In short, look at <a href="https://gamepadla.com/">gamepedla</a>, which has timings for
many controllers.</p>

<p>But I chose the <a href="https://www.8bitdo.com/ultimate-2-wireless-controller/">8bitdo Ultimate
2</a>. At a 1000hz polling
rate and measured sub 5ms latency for both wired and via 2.4Ghz, It’ll ensure a
lag-free experience if needed. In addition, the 10ms bluetooth latency isn’t too
bad!</p>

<p>While I’m waiting for that, I had an Xbox Core Controller lying around, and the
latency there <a href="https://gamepadla.com/xbox-core-controller.html">isn’t bad at
all</a>. Even on bluetooth it’s
11-12ms, comparable to bluetooth on the 8bitdo Ultimate 2.</p>]]></content><author><name></name></author><category term="gaming" /><summary type="html"><![CDATA[Finding a low latency game controller]]></summary></entry><entry><title type="html">My tips on debugging</title><link href="https://y.tsutsumi.io/debugging" rel="alternate" type="text/html" title="My tips on debugging" /><published>2025-06-07T07:00:00+00:00</published><updated>2025-06-07T07:00:00+00:00</updated><id>https://y.tsutsumi.io/debugging</id><content type="html" xml:base="https://y.tsutsumi.io/debugging"><![CDATA[<h1 id="my-tips-on-debugging">My tips on debugging</h1>

<p>Updated: 2025-06-07</p>

<p>This is a running collection of my thoughts on debugging. I’ll update these over time.</p>

<h2 id="improve-errors-minimize-print-statements">Improve errors, minimize print statements</h2>

<p>One common type of debugging is “print” style - just print the data you need at the time. Often this occurs because you don’t have enough information in the error itself:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"conflicting args"</span><span class="p">)</span>
</code></pre></div></div>

<p>Rather than add a print statement temporarily, just make the error better - it helps the next person when they come along:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s">"conflict args. Args found: </span><span class="si">{</span><span class="n">args</span><span class="si">}</span><span class="s">)
</span></code></pre></div></div>]]></content><author><name></name></author><category term="coding" /><summary type="html"><![CDATA[My tips on debugging]]></summary></entry><entry><title type="html">Result, Situation, Action: a new take on the STAR format</title><link href="https://y.tsutsumi.io/rsa" rel="alternate" type="text/html" title="Result, Situation, Action: a new take on the STAR format" /><published>2025-02-08T07:00:00+00:00</published><updated>2025-02-08T07:00:00+00:00</updated><id>https://y.tsutsumi.io/result-situation-action</id><content type="html" xml:base="https://y.tsutsumi.io/rsa"><![CDATA[<h1 id="result-situation-action">Result, Situation, Action</h1>

<p>The <a href="https://en.wikipedia.org/wiki/Situation,_task,_action,_result">STAR</a> format
of answering an interview question is very popular. But in my experience a twist
on that ends up being more effective: result, situation, then action.</p>

<p>The STAR format kind of buries the lead - it forces the interviewer into a
long-winded story and they’re not engaged because they don’t understand the
value of it. By giving them some idea of the impact that your story had (e.g. I
saved the company $XXX dollars, we reduced developer wait times by 20 minutes on
a 1-hour task), that gives the interviewer a reason to listen.</p>

<p>In addition, I find the “task” part of the story tends to be extraneous - when
you only have 30 minutes to an hour with the interviewer <em>total</em>, you need to
shave the minutes where it matters.</p>

<p>Although the whole “my task was X” may take 1 minute to explain, it’s better to just explain what you <em>did</em>.</p>

<h2 id="example">Example</h2>

<p>Result: I increased revenue for the company by XX percent.</p>

<p>Situation: In multiple customer meetings, I noticed that in the follow-up of
customers meetings, they would always mention having trouble onboarding onto
feature X.</p>

<p>Action: I wrote up a user guide, and we added a link the first time a customer
accessed the feature, as well as included it in the onboarding guide.</p>

<h2 id="summary">Summary</h2>

<p>I call this “RSA” pronounced “rizza”. Try it out!</p>]]></content><author><name></name></author><category term="coding" /><summary type="html"><![CDATA[Result, Situation, Action]]></summary></entry><entry><title type="html">Many and granular: ideal code organization in Bazel repos</title><link href="https://y.tsutsumi.io/bazel-many-and-granular" rel="alternate" type="text/html" title="Many and granular: ideal code organization in Bazel repos" /><published>2025-01-29T07:00:00+00:00</published><updated>2025-01-29T07:00:00+00:00</updated><id>https://y.tsutsumi.io/bazel-many-and-granular</id><content type="html" xml:base="https://y.tsutsumi.io/bazel-many-and-granular"><![CDATA[<h1 id="better-code-organization-in-bazel">Better code organization in bazel</h1>

<p>As part of my work on <a href="https://y.tsutsumi.io/depsaw/">depsaw</a>, I did a lot of
investigation into scenarios where we were overbuilding, and the solution for
those. Here’s some thoughts on code organization based on that exploration.</p>

<h2 id="more-granular-packages-are-better-than-fewer-packages">More granular packages are better than fewer packages</h2>

<p>The original post has <a href="https://y.tsutsumi.io/depsaw/#2-causes-and-solutions">a few scenarios
enumerated</a> of why
something is overbuilding, but I’ll repeat them here:</p>

<ol>
  <li>Too many files in a single package: this causes the package to change more
  frequently, but the dependant only needs a couple of the files.</li>
  <li>Too much functionality in a single file: there might be a file with thousands
  of lines of code, but only one function is relevant to a dependent.</li>
  <li>Unnecessary dependencies. Where a target has a dependency it doesn’t use at
all.</li>
</ol>

<p>Zooming in one (1) and (2) for a second: I think this points to a more
fundamental law about code organization in bazel: that packages should be “many
and granular”. Aside from just an incorrect declaration, the other solution to
overbuilding is basically “split the package”. Implying that if the dependencies
were granular to begin with, we would have preemptively solved overbuilding.</p>

<p>This is also outlined in the bazel <a href="https://bazel.build/configure/best-practices">best
practices</a>:</p>

<blockquote>
  <p>To use fine-grained dependencies to allow parallelism and incrementality.</p>
</blockquote>

<h2 id="a-shallow-hierarchy-is-better-than-a-deeper-one">A shallow hierarchy is better than a deeper one</h2>

<p>Although tied to the above, but still worth explicitly stating: a shallow
hierarchy is better than a deep one. For example,</p>

<pre><code class="language-mermaid">graph LR
    C --&gt; A
    C --&gt; B
    D --&gt; A
    D --&gt; B
    E --&gt; A
    F --&gt; B
</code></pre>

<p>Is better than:</p>

<pre><code class="language-mermaid">graph LR
    B --&gt; A
    C --&gt; A
    D --&gt; B
    E --&gt; B
    F --&gt; B
</code></pre>

<p>A shallow hierarchy:</p>

<ul>
  <li>Helps prevent pulling in functionality from upstream dependencies (better
depend-on-what-you-use).</li>
  <li>Makes it easier to sever and refactor dependencies (fewer layers to have to
navigate through).</li>
</ul>

<h2 id="summary">Summary</h2>

<ul>
  <li>more packages are better than fewer packages.</li>
  <li>granular, composable functionality is better than giant functionality.</li>
  <li>a shallow hierarchy of dependencies is better than a deeper one.</li>
</ul>]]></content><author><name></name></author><category term="coding" /><summary type="html"><![CDATA[Better code organization in bazel]]></summary></entry><entry><title type="html">Reflecting on 2024</title><link href="https://y.tsutsumi.io/my-year/2024" rel="alternate" type="text/html" title="Reflecting on 2024" /><published>2025-01-03T07:00:00+00:00</published><updated>2025-01-03T07:00:00+00:00</updated><id>https://y.tsutsumi.io/my-year/reflecting-on-2024</id><content type="html" xml:base="https://y.tsutsumi.io/my-year/2024"><![CDATA[<p>I’m reflecting on my personal time during 2024. A lot of my time was spent
working on things with my family that I won’t share here, but nontheless I did
make some progress with my personal goals.</p>

<h2 id="farsi">Farsi</h2>

<p>This year, I think my Farsi has gotten quite a bit better:</p>

<ul>
  <li>Conversationally I can express most things.</li>
  <li>I can watch some Farsi TV and news and get the general idea.</li>
</ul>

<p>The only video game with a Farsi translation I found is <a href="https://childrenofmorta.com/">Children of
Morta</a>: I guess one of the developers is Iranian
and wanted to get a Farsi translation in. I’m super thankful for that, so I can watch those cutscenes a few times and learn the Farsi from there.</p>

<p>The big thing I did is start listening to TV in the car regularly: it’s a good
way to reinforce my listening abilities.</p>

<p>I still take Farsi lessons weekly. For 2025, I think I’ll continue at this pace,
and loosely target being able to listen and understand Farsi TV.</p>

<h2 id="fitness-and-health">Fitness and Health</h2>

<p>I’ve continued working on fitness this year. Primarily lowering body fat,
building muscle, and lowering my apob.</p>

<p>I haven’t taken a Dexa scan since July, but my body fat actually went up at the
time compared to January: I think it was due to a two-week long trip I took to
Japan in June. I was not able to work out at all, and also walked a significant
amount of time so I was still, somewhat, in a caloric deficit.</p>

<p>That said, my weight at the end of the year is about 174 pounds. Losing roughly
4 pounds in the first half of the year to 176, and 2 pounds in the last half.
I’m not sure where my body fat is at, but likely 13% where it stood in July.</p>

<p>Interestingly I lost muscle mass across the board except for my stomach in July.
I still need to check to see if I was able to regain it or not.</p>

<p>I’ve found a good routine for a caloric deficit and overall weight loss, which
is:</p>

<ul>
  <li>400 calories daily burned at the treadmill (walking 30 minutes at 3.5mph, at a
12 degree incline).</li>
  <li>A 30 minute weightlifting workout, mixing in some crossfit-style workouts from
time to time to do some interval training.</li>
  <li>Maintaining ~1400 calories a day.</li>
</ul>

<p>I cheat on the calories regularly, so my weight is yo-yoing for now, but I’m
able to keep it between 171 and 175, and able to lose a pound a week with the
above. Given the holidays, I think this is invitable, but hoping to get back on
the wagon next year.</p>

<p>I upped my protein to 1.6g protein / kg, which I’ve read is about where the
benefits to hypertrophy level off.</p>

<p>For 2025, with a consistent technique to lose weight, I’m shifting my focus to
figure out how to build more muscle. I think there’s a lot of optimization I’m
missing out on there.</p>

<h2 id="japanese">Japanese</h2>

<p>For this year, my Japanese took a back burner. I stopped taking regular Japanese
lessons. Honestly I think to maximize those lessons, I would have to start doing
some pretty complex assignments like writing of papers and whatnot. It’s not
something I’m that interested in investing time into.</p>

<p>For now, I’ll just play video games in Japanese when I can. I have Witcher 3 and
the Tactics Ogre remake, which will be dozens of hours and are already
challenging me with new vocabulary.</p>

<h2 id="oss-aepdev">OSS: aep.dev</h2>

<p>aep.dev made a ton of progress! Don’t just take my word for it -  check out our
<a href="https://aep.dev/blog/2024-in-review/">public blog post</a> on the topic.</p>

<p>I’m very proud of the fact that we finally moved from just theoretical benefits
to the client tooling, to real UIs and clients. If you author an AEP-compliant
specification, you can get a <a href="https://github.com/aep-dev/aepcli">command line for
free</a>. We’re working on a web
<a href="https://github.com/aep-dev/aep-explorer">UI</a>, and have some prototyping of a
Terraform/OpenTofu provider as well.</p>

<p>With the progress we’ve made this year, I’m very excited about using that as a
foundation for continued progress in aep.dev in 2025. This could be the year we
really build out a full offering, and provide something compelling to adopters
of our API specification!</p>

<h2 id="goals-for-2025">Goals for 2025</h2>

<p>For 2025, my goals are still to primarily have ample time to do impromptu things
with my family. But I do have a few goals:</p>

<ul>
  <li>Get a roadmap for aep.dev</li>
  <li>Play piano 10 minutes a day. Last year, I did not learn new 5 piano pieces.
Darn! But I’ll make this my new goal for the year. My goal this year is to be
able to play piano pieces from just reading sheet music.</li>
  <li>Be able to understand persian news.</li>
  <li>Achieve 11% body fat, without losing muscle mass.</li>
</ul>]]></content><author><name></name></author><category term="2024" /><category term="reflection" /><summary type="html"><![CDATA[I’m reflecting on my personal time during 2024. A lot of my time was spent working on things with my family that I won’t share here, but nontheless I did make some progress with my personal goals.]]></summary></entry></feed>